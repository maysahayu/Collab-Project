{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVSRoYKIgJA52Le0lhW7F+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maysahayu/Collab-Project/blob/main/scrap_prepro_fe_cluster_Webtoon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8ckxbrvmAaM1"
      },
      "outputs": [],
      "source": [
        "import requests as req\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import pandas as pd\n",
        "import csv\n",
        "import os\n",
        "hades = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36'}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SCRAPING DATA\n",
        "\n",
        "def scraping_springer():\n",
        "    data_list = []\n",
        "    base_url = \"https://www.webtoons.com/id/\"\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    genres = [\"drama\", \"fantasy\", \"romantic_fantasy\", \"comedy\", \"action\", \"slice_of_life\", \"romance\", \"thriller\", \"horror\", \"local\"]\n",
        "\n",
        "    for page in genres:\n",
        "        url = f\"https://www.webtoons.com/id/genres/{page}?sortOrder=READ_COUNT\"\n",
        "\n",
        "        try:\n",
        "            halaman_utama = req.get(url, headers=headers).text\n",
        "            soup = bs(halaman_utama, 'lxml')\n",
        "\n",
        "            comics = soup.select('ul.card_lst a.card_item')\n",
        "\n",
        "            if not comics:\n",
        "                print(f\"Tidak ada artikel ditemukan di halaman {page}.\")\n",
        "                continue\n",
        "\n",
        "            for comic in comics:\n",
        "                judul_element = comic.find('p', class_='subj')\n",
        "                judul = judul_element.text.strip() if judul_element else 'Judul tidak ditemukan'\n",
        "\n",
        "                author_element = comic.find('p', class_='author')\n",
        "                author = author_element.text.strip() if author_element else 'Author tidak ditemukan'\n",
        "\n",
        "                # URL halaman detail komik\n",
        "                detail_url = comic['href']\n",
        "\n",
        "                # Masuk ke halaman detail untuk mengambil ringkasan\n",
        "                try:\n",
        "                    detail_page = req.get(detail_url, headers=headers).text\n",
        "                    detail_soup = bs(detail_page, 'lxml')\n",
        "\n",
        "                    # Cari elemen ringkasan di halaman detail\n",
        "                    desc_element = detail_soup.find('p', class_='summary')\n",
        "                    desc = desc_element.text.strip() if desc_element else 'Ringkasan tidak ditemukan'\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Terjadi kesalahan saat mengakses {detail_url}: {e}\")\n",
        "                    desc = 'Ringkasan tidak ditemukan'  # Menangani jika halaman detail gagal diakses\n",
        "\n",
        "                # Menambahkan data komik yang berhasil di-scrape\n",
        "                data_list.append([judul, author, desc])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Terjadi kesalahan saat mengakses {url}: {e}\")\n",
        "\n",
        "    # Menampilkan hasil scraping dalam bentuk DataFrame\n",
        "    df = pd.DataFrame(data_list, columns=['Judul Komik', 'Author', 'Ringkasan'])\n",
        "    print(df)\n",
        "\n",
        "    # Menyimpan DataFrame ke dalam file CSV\n",
        "    df.to_csv('data_webtoon.csv', index=False, encoding='utf-8')\n",
        "\n",
        "    print(\"Data berhasil disimpan\")\n"
      ],
      "metadata": {
        "id": "O2Smvp24AdwR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scraping_springer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVuNBRgkAgMK",
        "outputId": "60c4e614-0cee-45f6-cecf-d1f87215a2d3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     Judul Komik                     Author  \\\n",
            "0                        LOOKISM               Taejoon Park   \n",
            "1                   Girl's World                    morangg   \n",
            "2                The Real Lesson  CHAE YONGTAEK / HAN GARAM   \n",
            "3     Undercover at Chaebol High                         AJ   \n",
            "4               GOOD/BAD FORTUNE               Ariel Duyung   \n",
            "...                          ...                        ...   \n",
            "1499      SCRAMBLED: Journeylism                Lintankleen   \n",
            "1500                Dracko Diary                   Indra AD   \n",
            "1501                Perhaps Mine         Cikakey / Candrasa   \n",
            "1502                 Summer Rain                  kiyoshin_   \n",
            "1503                  Oh My Boy!                     Kanipa   \n",
            "\n",
            "                                              Ringkasan  \n",
            "0        Cowok ini punya 2 wujud! Pilih yang mana ya..?  \n",
            "1      Jadi bebek di antara para angsa? We will surive!  \n",
            "2     Setelah undang-undang larangan memukul para si...  \n",
            "3     Lee Dakyung, anggota sebuh organisasi gelap, t...  \n",
            "4     Hati-hati! Jika bertemu mereka, nasib kalian a...  \n",
            "...                                                 ...  \n",
            "1499  Visi adalah seorang siswi SMA yang pemalu. Saa...  \n",
            "1500  Dracko kini pergi merantau demi mengejar cita-...  \n",
            "1501  Mine yang baru saja dipecat karena suatu insid...  \n",
            "1502  Banyak hal aneh terjadi saat hujan panas. Sala...  \n",
            "1503  Hanya karena pendek, berwajah manis dan mengge...  \n",
            "\n",
            "[1504 rows x 3 columns]\n",
            "Data berhasil disimpan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PRE PROCESSING DATA\n",
        "\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import emoji\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# File paths\n",
        "file_path = \"data_webtoon.csv\"\n",
        "output_path = \"prepro_webtoon.csv\"\n",
        "kamus_alay = \"colloquial-indonesian-lexicon.csv\"\n",
        "\n",
        "kolom_target = 'Ringkasan'  # Kolom target yang akan diproses\n",
        "\n",
        "# Mendapatkan daftar stopwords\n",
        "stop_words = set(stopwords.words('indonesian'))\n",
        "\n",
        "# Fungsi untuk menghilangkan tanda baca\n",
        "def remove_punctuation(text):\n",
        "    if isinstance(text, str):\n",
        "        return text.translate(str.maketrans('', '', string.punctuation))\n",
        "    return text\n",
        "\n",
        "# Fungsi untuk menghilangkan URL dari teks\n",
        "def remove_url(text):\n",
        "    if isinstance(text, str):\n",
        "        url_pattern = r'http\\S+|www\\S+'\n",
        "        return re.sub(url_pattern, '', text)\n",
        "    return text\n",
        "\n",
        "# Fungsi untuk menghilangkan HTML tags dari teks\n",
        "def remove_html_tags(text):\n",
        "    if isinstance(text, str):\n",
        "        return re.sub(r'<.*?>', '', text)  # Menghapus semua yang ada di antara <>\n",
        "    return text\n",
        "\n",
        "# Fungsi untuk menghapus stopwords\n",
        "def remove_stopwords(text):\n",
        "    if isinstance(text, str):\n",
        "        words = re.findall(r'\\b\\w+\\b', text)  # Mengambil kata-kata yang valid\n",
        "        return ' '.join([word for word in words if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "def remove_digits(text):\n",
        "    if isinstance(text, str):\n",
        "        return re.sub(r'\\d+', '', text)  # Menghapus semua digit\n",
        "    return text\n",
        "\n",
        "# Fungsi untuk mengonversi emoji menjadi teks deskripsi sederhana\n",
        "def convert_emoji_to_text(text):\n",
        "    if isinstance(text, str):\n",
        "        return emoji.demojize(text)  # Mengonversi emoji menjadi teks\n",
        "    return text\n",
        "\n",
        "# Fungsi untuk menghilangkan simbol yang tidak diinginkan\n",
        "def remove_special_characters(text):\n",
        "    if isinstance(text, str):\n",
        "        return text.replace(\"ð\", \"\").replace(\"â\", \"\").replace(\"º\", \"\").replace(\"ï\", \"\")\n",
        "    return text\n",
        "\n",
        "# Membaca file Kamus Alay dan membuat dictionary alay -> formal\n",
        "df_slang = pd.read_csv(kamus_alay)\n",
        "alay_dict = dict(zip(df_slang['slang'], df_slang['formal']))\n",
        "\n",
        "# Fungsi untuk mengganti kata alay dengan kata formal\n",
        "def normalize_alay(text):\n",
        "    if isinstance(text, str):\n",
        "        return ' '.join([alay_dict.get(word, word) for word in text.split()])\n",
        "    return text\n",
        "\n",
        "# Membaca file CSV\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Mengubah teks menjadi lowercase pada kolom target\n",
        "data[kolom_target] = data[kolom_target].str.lower()\n",
        "\n",
        "# Menghapus URL terlebih dahulu\n",
        "data[kolom_target] = data[kolom_target].apply(remove_url)\n",
        "\n",
        "# Menghapus HTML tags setelah URL dihilangkan\n",
        "data[kolom_target] = data[kolom_target].apply(remove_html_tags)\n",
        "\n",
        "# Menghapus tanda baca setelah URL dihilangkan\n",
        "data[kolom_target] = data[kolom_target].apply(remove_punctuation)\n",
        "\n",
        "# Mengonversi emoji menjadi deskripsi teks sederhana\n",
        "data[kolom_target] = data[kolom_target].apply(convert_emoji_to_text)\n",
        "\n",
        "# Menghilangkan simbol yang tidak diinginkan dari teks\n",
        "data[kolom_target] = data[kolom_target].apply(remove_special_characters)\n",
        "\n",
        "# Menghapus digit angka\n",
        "data[kolom_target] = data[kolom_target].apply(remove_digits)\n",
        "\n",
        "# Menghapus stopwords\n",
        "data[kolom_target] = data[kolom_target].apply(remove_stopwords)\n",
        "\n",
        "# Mengganti kata-kata alay dengan kata formal\n",
        "data[kolom_target] = data[kolom_target].apply(normalize_alay)\n",
        "\n",
        "# Menyimpan hasilnya kembali ke file CSV\n",
        "data.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Semua teks telah diubah ke huruf kecil, URL, tanda baca, emoji telah dikonversi, stopwords, dan kata alay pada kolom '{kolom_target}' telah dihapus/diganti, dan file disimpan sebagai '{output_path}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coLEPxtbOIom",
        "outputId": "e4b9c8fe-be93-4ed1-dd15-11d27bd28021"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Semua teks telah diubah ke huruf kecil, URL, tanda baca, emoji telah dikonversi, stopwords, dan kata alay pada kolom 'Ringkasan' telah dihapus/diganti, dan file disimpan sebagai 'prepro_webtoon.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FEATURE ENGINEERING (BOW)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "cleaned_file_path = \"prepro_webtoon.csv\"\n",
        "output_bow_path = \"fe_webtoon.csv\"\n",
        "\n",
        "df_cleaned = pd.read_csv(cleaned_file_path)\n",
        "\n",
        "df_cleaned['Ringkasan'] = df_cleaned['Ringkasan'].fillna('')\n",
        "\n",
        "df_cleaned['Ringkasan'] = df_cleaned['Ringkasan'].astype(str)\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(df_cleaned['Ringkasan'])\n",
        "\n",
        "bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "\n",
        "bow_df.to_csv(output_bow_path, index=False)\n",
        "\n",
        "print(f\"Hasil Bag of Words telah disimpan ke dalam file '{output_bow_path}'\")\n",
        "\n",
        "print(bow_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASQO3zFnNIUt",
        "outputId": "6630be58-2665-4178-e4ce-72b501c40c39"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hasil Bag of Words telah disimpan ke dalam file 'fe_webtoon.csv'\n",
            "   aaron  abad  abadi  abang  abi  absurd  acara  acaraacara  achelans  \\\n",
            "0      0     0      0      0    0       0      0           0         0   \n",
            "1      0     0      0      0    0       0      0           0         0   \n",
            "2      0     0      0      0    0       0      0           0         0   \n",
            "3      0     0      0      0    0       0      0           0         0   \n",
            "4      0     0      0      0    0       0      0           0         0   \n",
            "\n",
            "   adakah  ...  yusin  zahra  zaman  zan  zara  zener  zenus  zeron  zion  \\\n",
            "0       0  ...      0      0      0    0     0      0      0      0     0   \n",
            "1       0  ...      0      0      0    0     0      0      0      0     0   \n",
            "2       0  ...      0      0      0    0     0      0      0      0     0   \n",
            "3       0  ...      0      0      0    0     0      0      0      0     0   \n",
            "4       0  ...      0      0      0    0     0      0      0      0     0   \n",
            "\n",
            "   zombie  \n",
            "0       0  \n",
            "1       0  \n",
            "2       0  \n",
            "3       0  \n",
            "4       0  \n",
            "\n",
            "[5 rows x 6284 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fDNtE5B8Qz28"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}